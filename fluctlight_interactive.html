<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Fluctlight AI - Interactive Visualizer</title>
    <link rel="icon" href="data:,"> 
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background-color: #000c1a; /* Dark blue background */
            color: #e0e0e0;
            margin: 0;
            padding: 0;
            /* display: flex; */ /* Removed flex properties to allow fixed positioning */
            /* flex-direction: column; */ /* Removed */
            /* align-items: center; */ /* Removed */
            /* justify-content: center; */ /* Removed */
            min-height: 100vh;
            overflow: hidden; /* Hide scrollbars if canvas is larger */
        }

        #particleCanvas {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            z-index: -1; /* Behind other content */
        }

        .container {
            position: fixed; /* Position it relative to the viewport */
            bottom: 20px;    /* Distance from the bottom */
            left: 20px;      /* Distance from the left */
            background-color: rgba(10, 25, 47, 0.85); /* Slightly transparent dark blue */
            padding: 20px; /* Adjusted padding */
            border-radius: 10px; /* Adjusted border-radius */
            box-shadow: 0 0 20px rgba(97, 218, 251, 0.25); /* Accent color glow */
            text-align: left; /* Align text to the left within the container */
            width: auto; /* Let content define width, or set a specific one */
            max-width: 400px; /* Max width for the container in the corner */
            z-index: 1;
        }

        h1 {
            color: #61dafb; /* Accent color */
            margin-top: 0; /* Remove top margin if it's the first element */
            margin-bottom: 15px; /* Adjusted margin */
            font-size: 1.5em; /* Adjusted font size */
        }

        #aiStatus {
            margin-bottom: 20px;
            font-size: 1.2em;
            color: #88ddff;
            min-height: 1.5em;
        }

        textarea#userInput {
            width: calc(100% - 22px);
            padding: 10px;
            margin-bottom: 15px;
            border-radius: 8px;
            border: 1px solid #61dafb;
            background-color: #001f3f; /* Darker blue for input */
            color: #e0e0e0;
            font-size: 1em;
            resize: vertical;
            min-height: 60px;
        }

        button {
            padding: 12px 25px;
            margin: 5px;
            border: none;
            border-radius: 8px;
            background-color: #61dafb; /* Accent color */
            color: #000c1a; /* Dark blue text on button */
            font-size: 1em;
            cursor: pointer;
            transition: background-color 0.3s ease, transform 0.1s ease;
        }

        button:hover {
            background-color: #88ddff; /* Lighter accent */
        }
        button:active {
            transform: scale(0.98);
        }

        #subtitles {
            position: fixed;
            top: 50%; /* Center vertically */
            left: 50%; /* Center horizontally */
            transform: translate(-50%, -50%); /* Precise centering */
            width: 80%; /* Responsive width */
            text-align: center;
            font-family: 'Times New Roman', Times, serif;
            font-size: 3em; /* Large text - adjust as needed */
            color: #f0f0f0; /* Light color for subtitles */
            opacity: 0.4;   /* Low opacity for the container */
            z-index: 0;     /* Above particleCanvas (-1), below UI container (1) */
            pointer-events: none; /* Allow clicks to pass through */
            display: none;  /* Hidden by default, JS will control visibility */
            text-shadow: 1px 1px 2px rgba(0,0,0,0.7); /* Slight shadow for readability */
        }

        #subtitles span { /* For individual word styling during animation */
            transition: opacity 0.2s ease-in-out;
            display: inline-block; /* Ensures spaces are handled correctly if needed */
        }


        /* Speaker ID Selector Styles */
        #speakerSelectorContainer {
            position: fixed;
            top: 20px;
            right: 20px;
            z-index: 100; /* Ensure it's above other elements */
            background-color: rgba(10, 25, 47, 0.85);
            padding: 10px;
            border-radius: 8px;
            box-shadow: 0 0 15px rgba(97, 218, 251, 0.2);
        }

        #speakerSelectorContainer label {
            color: #61dafb;
            margin-right: 10px;
            font-size: 0.9em;
        }

        #speakerIdSelect {
            background-color: #001f3f;
            color: #e0e0e0;
            border: 1px solid #61dafb;
            border-radius: 5px;
            padding: 5px;
            font-size: 0.9em;
        }
    </style>
</head>
<body>
    <canvas id="particleCanvas"></canvas>
    <div id="subtitles"></div>

    <div id="speakerSelectorContainer">
        <label for="speakerIdSelect">Speaker ID:</label>
        <select id="speakerIdSelect"></select>
    </div>

    <div class="container">
        <h1>Fluctlight AI</h1>
        <div id="aiStatus">Initializing...</div>
        <textarea id="userInput" placeholder="Type your message to Fluctlight AI..."></textarea>
        <button id="sendButton">Send</button>
        <button id="recordButton">Record Voice</button>
        <button id="generateImage">Generate Image</button>
        <button id="generateMusic">Create Music</button>
    </div>

    <script>
        document.addEventListener('DOMContentLoaded', () => {
            const aiStatus = document.getElementById('aiStatus');
            const userInput = document.getElementById('userInput');
            const sendButton = document.getElementById('sendButton');
            const recordButton = document.getElementById('recordButton');
            const subtitlesDiv = document.getElementById('subtitles');
            const speakerIdSelect = document.getElementById('speakerIdSelect');
            
            // Particle Canvas Setup
            const particleCanvas = document.getElementById('particleCanvas');
            const pCtx = particleCanvas.getContext('2d');
            particleCanvas.width = window.innerWidth;
            particleCanvas.height = window.innerHeight;

            // Removed particlesArray and mouse object related to particles

            // --- New variables for central particles ---
            let centerParticles = [];
            const MAX_CENTER_PARTICLES = 150; // Max particles for the center
            const PARTICLE_COLORS = ['#FF61D8', '#61FFD8', '#FFD861', '#61D8FF', '#D8FF61', '#FFFFFF']; // Vibrant colors + white

            // --- New variables for interactive grid ---
            const gridSpacing = 40; // Spacing of grid lines in pixels
            const gridHighlightDistance = 20; // How close mouse needs to be to highlight a line
            const defaultGridColor = 'rgba(97, 218, 251, 0.03)'; // Very subtle default grid color
            const highlightGridColor = 'rgba(255, 255, 255, 0.5)'; // Highlighted grid line color
            let mouse = { x: undefined, y: undefined }; // To store mouse coordinates

            // --- New variables for Recording ---
            let mediaRecorder;
            let audioChunks = [];
            let isRecording = false;
            // const OPENROUTER_WHISPER_URL = "https://openrouter.ai/api/v1/audio/transcriptions"; // Commented out
            const OPENAI_API_KEY = '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'; // Your OpenAI API Key
            const OPENAI_WHISPER_API_URL = "https://api.openai.com/v1/audio/transcriptions";


            // --- New Particle Class ---
            class Particle {
                constructor(x, y, size, color, velocityX, velocityY) {
                    this.x = x;
                    this.y = y;
                    this.size = Math.max(0.5, size); // Ensure size is not too small initially
                    this.initialSize = this.size;
                    this.color = color;
                    this.velocityX = velocityX;
                    this.velocityY = velocityY;
                    this.life = 1; // Represents full life, decreases to 0
                    this.decay = Math.random() * 0.008 + 0.002; // Random decay rate for lifespan
                }

                update() {
                    this.x += this.velocityX;
                    this.y += this.velocityY;
                    
                    // Optional: Add some friction or gravity effect
                    // this.velocityX *= 0.99;
                    // this.velocityY *= 0.99;
                    // this.velocityY += 0.01; // Tiny gravity

                    this.life -= this.decay;
                    this.size = this.initialSize * this.life; // Size shrinks with life

                    // Keep particles within a certain radius from the center, or bounce them
                    const distanceFromCenter = Math.sqrt(Math.pow(this.x - visualizerCenterX, 2) + Math.pow(this.y - visualizerCenterY, 2));
                    if (distanceFromCenter > visualizerInnerRadius * 1.5) { // Example boundary
                         this.life = 0; // Or make them bounce
                    }
                }

                draw(ctx) {
                    if (this.life <= 0 || this.size <= 0.1) return;

                    ctx.beginPath();
                    // Make color more transparent as life decreases
                    const alpha = Math.max(0, this.life * 0.8).toFixed(2); // Ensure alpha is between 0 and 0.8
                    let particleColor = this.color;
                    if (particleColor.startsWith('#')) { // Convert hex to rgba for opacity
                        const r = parseInt(particleColor.slice(1, 3), 16);
                        const g = parseInt(particleColor.slice(3, 5), 16);
                        const b = parseInt(particleColor.slice(5, 7), 16);
                        particleColor = `rgba(${r}, ${g}, ${b}, ${alpha})`;
                    } else if (particleColor.startsWith('rgb(')) {
                        particleColor = particleColor.replace('rgb(', 'rgba(').replace(')', `, ${alpha})`);
                    } else if (particleColor.startsWith('rgba(')) {
                         // If already rgba, update alpha (this is a bit simplistic, might need better parsing)
                        particleColor = particleColor.replace(/,\s*\d?\.?\d*\s*\)/, `, ${alpha})`);
                    }
                    
                    ctx.fillStyle = particleColor;
                    ctx.arc(this.x, this.y, Math.max(0, this.size), 0, Math.PI * 2, false);
                    ctx.fill();
                }
            }

            // Visualizer properties
            let visualizerCenterX = particleCanvas.width / 2;
            let visualizerCenterY = particleCanvas.height / 2;
            // Radius for the inner part of the visualizer, bars will extend outwards from this
            let visualizerInnerRadius = Math.min(particleCanvas.width, particleCanvas.height) / 8; 
            let maxBarLength = Math.min(particleCanvas.width, particleCanvas.height) / 3;


            // Removed mousemove event listener for particles
            window.addEventListener('resize', () => {
                particleCanvas.width = window.innerWidth;
                particleCanvas.height = window.innerHeight;
                visualizerCenterX = particleCanvas.width / 2;
                visualizerCenterY = particleCanvas.height / 2;
                visualizerInnerRadius = Math.min(particleCanvas.width, particleCanvas.height) / 8;
                maxBarLength = Math.min(particleCanvas.width, particleCanvas.height) / 3;
                // No need to re-initialize particles
            });

            // --- Add Mouse Listeners for Interactive Grid ---
            particleCanvas.addEventListener('mousemove', (event) => {
                const rect = particleCanvas.getBoundingClientRect();
                mouse.x = event.clientX - rect.left;
                mouse.y = event.clientY - rect.top;
            });

            particleCanvas.addEventListener('mouseleave', () => {
                mouse.x = undefined;
                mouse.y = undefined;
            });

            // Removed Particle class

            // Web Audio API Setup (mostly unchanged, ensure analyserNode is used for frequency data)
            let audioContext;
            let analyserNode;
            let microphoneSourceNode; 
            let aiAudioSourceNode;    
            let amplitudeDataArray; // This will now store frequency data
            // let globalVoiceAmplitude = 0; // Removed, we'll use the full frequency array

            async function initAudioContext() {
                if (!audioContext) {
                    audioContext = new (window.AudioContext || window.webkitAudioContext)();
                }

                if (audioContext.state === 'suspended') {
                    await audioContext.resume();
                }

                if (!analyserNode) {
                    analyserNode = audioContext.createAnalyser();
                    analyserNode.fftSize = 256; 
                    const bufferLength = analyserNode.frequencyBinCount;
                    amplitudeDataArray = new Uint8Array(bufferLength); 
                }

                if (!microphoneSourceNode) {
                    try {
                        const stream = await navigator.mediaDevices.getUserMedia({ audio: true, video: false });
                        microphoneSourceNode = audioContext.createMediaStreamSource(stream);
                        microphoneSourceNode.connect(analyserNode); // Connect to analyser for visualizer
                        // Do NOT connect microphoneSourceNode directly to audioContext.destination if you only want it for recording/visualizing
                        console.log("Microphone connected to analyser for visualizer and ready for recording.");
                    } catch (err) {
                        console.error('Error accessing microphone:', err);
                        aiStatus.textContent = 'Microphone access denied or error.';
                        throw err; // Re-throw to prevent further actions if mic fails
                    }
                }
            }
            
            sendButton.addEventListener('click', async () => {
                if (!audioContext || audioContext.state === 'suspended' || !microphoneSourceNode) {
                    try {
                        await initAudioContext(); 
                    } catch (error) {
                        console.error("Failed to init audio for send:", error);
                        aiStatus.textContent = "Mic setup failed. Cannot send.";
                        return;
                    }
                }
                // Existing send logic will follow
            });

            // --- Recording Logic ---
            async function startRecording() {
                if (isRecording) return;

                try {
                    if (!audioContext || audioContext.state === 'suspended' || !microphoneSourceNode) {
                        await initAudioContext();
                    }
                     // Ensure microphoneSourceNode.stream is available
                    if (!microphoneSourceNode || !microphoneSourceNode.mediaStream) {
                        console.error("Microphone stream not available.");
                        aiStatus.textContent = "Mic stream error. Cannot record.";
                        return;
                    }

                    audioChunks = [];
                    // Use the stream directly from the microphoneSourceNode
                    mediaRecorder = new MediaRecorder(microphoneSourceNode.mediaStream);

                    mediaRecorder.ondataavailable = event => {
                        audioChunks.push(event.data);
                    };

                    mediaRecorder.onstop = async () => {
                        const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
                        audioChunks = [];
                        await sendAudioForTranscription(audioBlob);
                        
                        // Add automatic send after transcription
                        if (userInput.value.trim()) {
                            document.getElementById('sendButton').click();
                        }
                        
                        recordButton.textContent = 'Record Voice';
                        isRecording = false;
                    };

                    mediaRecorder.start();
                    isRecording = true;
                    recordButton.textContent = 'Stop Recording';
                    aiStatus.textContent = 'Recording...';
                } catch (error) {
                    console.error('Error starting recording:', error);
                    aiStatus.textContent = 'Failed to start recording.';
                    isRecording = false; // Reset state
                    recordButton.textContent = 'Record Voice';
                }
            }

            function stopRecording() {
                if (!isRecording || !mediaRecorder) return;
                mediaRecorder.stop();
                // isRecording and button text will be updated in mediaRecorder.onstop
                aiStatus.textContent = 'Processing audio...';
            }

            async function sendAudioForTranscription(audioBlob) {
                aiStatus.textContent = 'Transcribing audio...';
                console.log('Audio Blob for transcription:', audioBlob.size, audioBlob.type); // Log blob size and type

                if (audioBlob.size === 0) {
                    aiStatus.textContent = 'Recorded audio is empty. Please try again.';
                    console.error('Audio blob is empty.');
                    return;
                }

                const formData = new FormData();
                formData.append('file', audioBlob, 'recording.webm'); // filename is important
                formData.append('model', 'whisper-1'); // Model for OpenAI API

                try {
                    const response = await fetch(OPENAI_WHISPER_API_URL, { // Use OpenAI URL
                        method: 'POST',
                        headers: {
                            "Authorization": `Bearer ${OPENAI_API_KEY}`, // Use OpenAI API Key
                            // "Content-Type": "multipart/form-data" // Fetch API sets this automatically for FormData
                        },
                        body: formData,
                    });

                    if (!response.ok) {
                        const errorData = await response.json().catch(() => ({ message: response.statusText })); // Provide fallback for non-JSON error
                        console.error("OpenAI Whisper API Error Response:", errorData);
                        console.error("OpenAI Whisper API Status:", response.status, response.statusText);
                        throw new Error(`OpenAI Whisper API request failed: ${response.status} ${response.statusText} - ${JSON.stringify(errorData.error || errorData)}`);
                    }

                    const data = await response.json();
                    const transcribedText = data.text; // Based on typical Whisper API response

                    if (transcribedText !== undefined) { // Check for undefined, as empty string is a valid transcription
                        userInput.value = transcribedText;
                        aiStatus.textContent = 'Transcription complete. Ready to send.';
                    } else {
                        aiStatus.textContent = 'Transcription returned no text.';
                        console.warn('OpenAI Whisper API returned no text field in response:', data);
                    }

                } catch (error) {
                    console.error('Error during OpenAI Whisper transcription request:', error);
                    aiStatus.textContent = 'Error transcribing audio with OpenAI. Check console.';
                }
            }

            recordButton.addEventListener('click', async () => {
                if (!isRecording) {
                    await startRecording();
                } else {
                    stopRecording();
                }
            });


            // Removed initParticles function
            // Removed animateParticles function
            // Removed triggerSpeechParticleEffect function

            // --- New function to draw interactive grid ---
            function drawInteractiveGrid(ctx, canvasWidth, canvasHeight) {
                ctx.save();
                ctx.lineWidth = 1;
                ctx.shadowBlur = 0; // Ensure no leftover shadow from other drawings

                // Vertical lines
                for (let x = 0; x <= canvasWidth; x += gridSpacing) {
                    const isHighlighted = mouse.x !== undefined && Math.abs(mouse.x - x) < gridHighlightDistance && mouse.y !== undefined;
                    ctx.strokeStyle = isHighlighted ? highlightGridColor : defaultGridColor;
                    if (isHighlighted) {
                        // Optional: add a subtle glow to highlighted lines
                        // ctx.shadowColor = 'white';
                        // ctx.shadowBlur = 3;
                    }
                    ctx.beginPath();
                    ctx.moveTo(x, 0);
                    ctx.lineTo(x, canvasHeight);
                    ctx.stroke();
                    // if (isHighlighted) ctx.shadowBlur = 0; // Reset shadow for next line
                }

                // Horizontal lines
                for (let y = 0; y <= canvasHeight; y += gridSpacing) {
                    const isHighlighted = mouse.y !== undefined && Math.abs(mouse.y - y) < gridHighlightDistance && mouse.x !== undefined;
                    ctx.strokeStyle = isHighlighted ? highlightGridColor : defaultGridColor;
                    // if (isHighlighted) {
                        // ctx.shadowColor = 'white';
                        // ctx.shadowBlur = 3;
                    // }
                    ctx.beginPath();
                    ctx.moveTo(0, y);
                    ctx.lineTo(canvasWidth, y);
                    ctx.stroke();
                    // if (isHighlighted) ctx.shadowBlur = 0;
                }
                ctx.restore();
            }

            // --- New function for central particles ---
            function updateAndDrawCenterParticles(ctx, centerX, centerY, audioData) {
                // Spawn new particles
                let averageAmplitude = 0;
                if (audioData && audioData.length > 0) {
                    // Use a slice of the frequency data, e.g., lower frequencies for more impact
                    const relevantData = audioData.slice(0, Math.floor(audioData.length / 4));
                    averageAmplitude = relevantData.reduce((sum, val) => sum + val, 0) / (relevantData.length || 1);
                }
                
                // Spawn rate and properties can be tied to averageAmplitude
                const spawnChance = 0.2 + (averageAmplitude / 255) * 0.8; // More particles with louder sound

                if (centerParticles.length < MAX_CENTER_PARTICLES && Math.random() < spawnChance) {
                    const angle = Math.random() * Math.PI * 2;
                    const speed = (Math.random() * 1.5 + 0.5) + (averageAmplitude / 255) * 1.5;
                    const size = (Math.random() * 2 + 1) + (averageAmplitude / 255) * 2.5;
                    const color = PARTICLE_COLORS[Math.floor(Math.random() * PARTICLE_COLORS.length)];
                    
                    centerParticles.push(new Particle(
                        centerX,
                        centerY,
                        size,
                        color,
                        Math.cos(angle) * speed,
                        Math.sin(angle) * speed
                    ));
                }

                // Update and draw existing particles
                for (let i = centerParticles.length - 1; i >= 0; i--) {
                    centerParticles[i].update();
                    if (centerParticles[i].life <= 0 || centerParticles[i].size <= 0.1) {
                        centerParticles.splice(i, 1);
                    } else {
                        centerParticles[i].draw(ctx);
                    }
                }
            }

            function animateVisualizer() {
                requestAnimationFrame(animateVisualizer);
                pCtx.clearRect(0, 0, particleCanvas.width, particleCanvas.height);

                // 1. Draw interactive grid first (as background)
                drawInteractiveGrid(pCtx, particleCanvas.width, particleCanvas.height);

                if (!analyserNode || !microphoneSourceNode || !amplitudeDataArray) {
                    // Draw a placeholder or idle state if audio isn't ready
                    // You could also draw a calmer version of the particles here
                    pCtx.fillStyle = 'rgba(97, 218, 251, 0.05)'; // Subtle idle circle
                    pCtx.beginPath();
                    pCtx.arc(visualizerCenterX, visualizerCenterY, visualizerInnerRadius * 0.5, 0, Math.PI * 2);
                    pCtx.fill();
                    // Also draw some idle center particles
                    updateAndDrawCenterParticles(pCtx, visualizerCenterX, visualizerCenterY, null); // Pass null or empty array for audioData
                    return;
                }

                analyserNode.getByteFrequencyData(amplitudeDataArray); // Get frequency data

                // 2. Draw the main frequency bars (existing code)
                const numBars = amplitudeDataArray.length * 0.8; // Use about 80% of available bins
                // const barWidth = (Math.PI * 2) / numBars / 2; // Width of each bar in radians (with some spacing) - This line seems unused if not drawing radial bars
                
                pCtx.strokeStyle = '#61dafb'; // Default bar color
                pCtx.shadowColor = '#61dafb';
                pCtx.shadowBlur = 5;

                for (let i = 0; i < numBars; i++) {
                    const barHeightNormalized = amplitudeDataArray[i] / 255; // Normalize to 0-1
                    const barHeight = barHeightNormalized * maxBarLength;

                    if (barHeight < 1) continue; // Don't draw tiny bars

                    const angle = (i / numBars) * Math.PI * 2 - Math.PI / 2; // Start from top

                    const startX = visualizerCenterX + visualizerInnerRadius * Math.cos(angle);
                    const startY = visualizerCenterY + visualizerInnerRadius * Math.sin(angle);
                    const endX = visualizerCenterX + (visualizerInnerRadius + barHeight) * Math.cos(angle);
                    const endY = visualizerCenterY + (visualizerInnerRadius + barHeight) * Math.sin(angle);

                    pCtx.beginPath();
                    pCtx.lineWidth = 2; // Adjust for desired thickness
                    // Vary color/intensity based on height
                    const intensity = Math.min(1, 0.2 + barHeightNormalized * 1.2);
                    pCtx.strokeStyle = `rgba(97, 218, 251, ${intensity})`;
                    pCtx.shadowColor = `rgba(97, 218, 251, ${intensity * 0.5})`;


                    pCtx.moveTo(startX, startY);
                    pCtx.lineTo(endX, endY);
                    pCtx.stroke();
                }
                pCtx.shadowBlur = 0; // Reset shadow for other elements if any

                // 3. Draw dynamic colorful particles in the center (replaces the old circle)
                updateAndDrawCenterParticles(pCtx, visualizerCenterX, visualizerCenterY, amplitudeDataArray);

                // Placeholder for central elements (draw a simple circle for now) - REMOVED
                // pCtx.fillStyle = 'rgba(97, 218, 251, 0.2)';
                // pCtx.beginPath();
                // pCtx.arc(visualizerCenterX, visualizerCenterY, visualizerInnerRadius * 0.8, 0, Math.PI * 2);
                // pCtx.fill();
            }

            // --- New function for animating subtitles word by word ---
            function animateTextDisplay(element, text, totalDuration) {
                element.innerHTML = ''; // Clear previous subtitles
                element.style.display = 'block';
                const words = text.trim().split(/\s+/);
                const numWords = words.length;

                if (numWords === 0) {
                    element.style.display = 'none';
                    return;
                }

                // Estimate duration per word, ensure it's not too fast
                const minDelayPerWord = 50; // Minimum ms per word to ensure readability
                let delayPerWord = totalDuration / numWords;
                if (numWords * minDelayPerWord > totalDuration) { // If calculated delay is too short
                    delayPerWord = minDelayPerWord;
                }


                words.forEach((word, index) => {
                    const wordSpan = document.createElement('span');
                    wordSpan.textContent = word + ' '; // Add space after each word
                    wordSpan.style.opacity = '0.5'; // Initially dimmer than the "active" word
                    element.appendChild(wordSpan);

                    setTimeout(() => {
                        wordSpan.style.opacity = '1'; // Highlight current word (fully opaque relative to parent)
                        
                        // Optionally, make previous words even dimmer after being read
                        if (index > 0) {
                            const previousWordSpan = element.children[index - 1];
                            if (previousWordSpan) {
                                // previousWordSpan.style.opacity = '0.3'; 
                            }
                        }
                    }, index * delayPerWord);
                });

                // Hide subtitles after the full duration + a small buffer
                const hideDelay = Math.max(totalDuration, numWords * minDelayPerWord);
                setTimeout(() => {
                    element.style.display = 'none';
                    element.innerHTML = ''; // Clear out spans
                }, hideDelay + 700); // Add a bit more buffer
            }


            async function playAudioAndDisplaySubtitles(audioUrl, fullResponseText, cleanedTextForSpeech) {
                if (aiAudioSourceNode) {
                    aiAudioSourceNode.disconnect();
                }
                if (!audioContext) {
                    await initAudioContext();
                }
                aiAudioSourceNode = audioContext.createBufferSource();
                
                try {
                    const response = await fetch(audioUrl);
                    const arrayBuffer = await response.arrayBuffer();
                    const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);

                    aiAudioSourceNode.buffer = audioBuffer;
                    aiAudioSourceNode.connect(audioContext.destination);
                    if (analyserNode) { // Also connect to analyser for visualizer if AI is speaking
                        aiAudioSourceNode.connect(analyserNode);
                    }
                    aiAudioSourceNode.start(0);
                    aiStatus.textContent = 'Fluctlight AI is speaking...';

                    // Use the new animation function for subtitles
                    const audioDuration = audioBuffer.duration * 1000; // Duration in milliseconds
                    animateTextDisplay(subtitlesDiv, cleanedTextForSpeech, audioDuration);


                    aiAudioSourceNode.onended = () => {
                        aiStatus.textContent = 'Ready.';
                        if (analyserNode && aiAudioSourceNode) { // Disconnect from analyser after speaking
                           // aiAudioSourceNode.disconnect(analyserNode); // This can cause issues if source is reused or already disconnected
                        }
                    };

                } catch (error) {
                    console.error('Error playing audio or displaying subtitles:', error);
                    aiStatus.textContent = 'Error playing audio.';
                    subtitlesDiv.style.display = 'none'; // Hide subtitles on error
                }
            }

            // Start the new visualizer
            // Ensure initAudioContext is called before starting, typically on user interaction
            // For now, let's assume sendButton click will init it.
            // If you want it to start immediately on load (after mic permission), you'd call initAudioContext then animateVisualizer.
            animateVisualizer();


            // --- Fluctlight AI Logic --- (largely unchanged below this point)
            const OPENROUTER_API_KEY = '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';
            const VOICEVOX_API_URL = 'http://localhost:50021';

            // The engToKatakanaMap might not be directly used for AI responses anymore,
            // but could be kept for other potential uses or removed if not needed.
            const engToKatakanaMap = {
                "you": "ユー", "said": "セッド", "i": "アイ", "am": "アム",
                "fluctlight": "フラクトライト", "ai": "エーアイ", "hello": "ハロー",
                "world": "ワールド", "is": "イズ", "are": "アー", "this": "ディス",
                "that": "ザット", "computer": "コンピューター",
            };

            function convertToKatakana(englishText) {
                if (!englishText) return "";
                const words = englishText.toLowerCase().replace(/[.,!?"']/g, '').split(/\s+/);
                const katakanaWords = words.map(word => {
                    if (word === "") return "";
                    return engToKatakanaMap[word] || word.toUpperCase();
                });
                return katakanaWords.filter(word => word !== "").join(' ');
            }

            async function getAIResponse(userText) {
                aiStatus.textContent = 'Thinking...';
                subtitlesDiv.style.display = 'none'; // Hide old subtitles

                const prompt = `あなたは「フラクトライトAI」という名前のAIアシスタントです。ユーザーは次のように述べています。「${userText}」\n\n応答は完全に日本語で行ってください。日本語の応答の後、改行して「English Subtitle:」という接頭辞を付けて、その日本語の応答の正確な英語字幕を提供してください。\n\n例:\nこんにちは、フラクトライトAIです。何かお手伝いできますか？\nEnglish Subtitle: Hello, I am Fluctlight AI. How can I help you?`;

                try {
                    const response = await fetch("https://openrouter.ai/api/v1/chat/completions", {
                        method: "POST",
                        headers: {
                            "Authorization": `Bearer ${OPENROUTER_API_KEY}`,
                            "Content-Type": "application/json"
                        },
                        body: JSON.stringify({
                            "model": "deepseek/deepseek-chat", // Or other DeepSeek model like deepseek/deepseek-coder
                            "messages": [
                                { "role": "user", "content": prompt }
                            ]
                        })
                    });

                    if (!response.ok) {
                        const errorData = await response.json();
                        console.error("OpenRouter API Error:", errorData);
                        throw new Error(`API request failed: ${response.status} ${response.statusText} - ${errorData.error?.message || 'Unknown error'}`);
                    }

                    const data = await response.json();
                    const aiContent = data.choices[0].message.content;

                    // Parse the AI content to separate Japanese response and English subtitle
                    const parts = aiContent.split("\nEnglish Subtitle:");
                    const japaneseResponse = parts[0].trim();
                    const englishSubtitle = parts[1] ? parts[1].trim() : "Sorry, I couldn't provide a subtitle.";

                    return { japaneseResponse, englishSubtitle };

                } catch (error) {
                    console.error('Error fetching AI response:', error);
                    aiStatus.textContent = 'Error connecting to AI. Try again.';
                    return { 
                        japaneseResponse: "申し訳ありませんが、現在応答できません。", 
                        englishSubtitle: "Sorry, I cannot respond right now." 
                    };
                }
            }


            sendButton.addEventListener('click', async () => {
                const text = userInput.value;
                if (text.trim() !== '') {
                    userInput.value = ''; // Clear input immediately
                    aiStatus.textContent = 'Processing...';
                    
                    const { japaneseResponse, englishSubtitle } = await getAIResponse(text);
                    
                    if (japaneseResponse) {
                        aiStatus.textContent = 'Speaking...';
                        subtitlesDiv.textContent = englishSubtitle;
                        subtitlesDiv.style.display = 'block';
                        speakWithVoicevox(japaneseResponse, englishSubtitle); // Pass subtitle here
                    } else {
                        aiStatus.textContent = 'Failed to get response.';
                        subtitlesDiv.style.display = 'none';
                    }
                }
            });

            userInput.addEventListener('keypress', (event) => {
                if (event.key === 'Enter' && !event.shiftKey) { // Allow shift+enter for newlines in textarea
                    event.preventDefault(); // Prevent default Enter behavior (newline)
                    sendButton.click();
                }
            });

            recordButton.addEventListener('click', () => {
                aiStatus.textContent = 'Recording feature is a work in progress.';
                // Placeholder for actual recording logic
            });

            let currentSpeakerId = 27; // Default speaker ID: Goki - Human ver.

            const speakerStyles = [
                { id: 0, character: "Shikoku Metan", style: "Sweet" },
                { id: 2, character: "Shikoku Metan", style: "Normal" },
                { id: 4, character: "Shikoku Metan", style: "Sexy" },
                { id: 6, character: "Shikoku Metan", style: "Tsundere" },
                { id: 36, character: "Shikoku Metan", style: "Whisper" },
                { id: 37, character: "Shikoku Metan", style: "Hushed" },
                { id: 1, character: "Zundamon", style: "Sweet" },
                { id: 3, character: "Zundamon", style: "Normal" },
                { id: 5, character: "Zundamon", style: "Sexy" },
                { id: 7, character: "Zundamon", style: "Tsundere" },
                { id: 22, character: "Zundamon", style: "Whisper" },
                { id: 38, character: "Zundamon", style: "Hushed" },
                { id: 8, character: "Kasukabe Tsumugi", style: "Normal" },
                { id: 9, character: "Namine Ritsu", style: "Normal" },
                { id: 10, character: "Amehare Hau", style: "Normal" },
                { id: 11, character: "Kurono Takehiro", style: "Normal" },
                { id: 39, character: "Kurono Takehiro", style: "Joyful" },
                { id: 40, character: "Kurono Takehiro", style: "Tsundere-Angry" },
                { id: 41, character: "Kurono Takehiro", style: "Sad" },
                { id: 12, character: "Shirakami Kotarou", style: "Normal" },
                { id: 32, character: "Shirakami Kotarou", style: "Excited" },
                { id: 33, character: "Shirakami Kotarou", style: "Nervous" },
                { id: 34, character: "Shirakami Kotarou", style: "Angry" },
                { id: 35, character: "Shirakami Kotarou", style: "Crying" },
                { id: 13, character: "Aoyama Ryusei", style: "Normal" },
                { id: 14, character: "Meimei Himari", style: "Normal" },
                { id: 15, character: "Kyushu Sora", style: "Sweet" },
                { id: 16, character: "Kyushu Sora", style: "Normal" },
                { id: 17, character: "Kyushu Sora", style: "Sexy" },
                { id: 18, character: "Kyushu Sora", style: "Tsundere" },
                { id: 19, character: "Kyushu Sora", style: "Whisper" },
                { id: 20, character: "Mochiko-san", style: "Normal" },
                { id: 21, character: "Kenzaki Mesuo", style: "Normal" },
                { id: 23, character: "WhiteCUL", style: "Normal" },
                { id: 24, character: "WhiteCUL", style: "Happy" },
                { id: 25, character: "WhiteCUL", style: "Sad" },
                { id: 26, character: "WhiteCUL", style: "Crying" },
                { id: 27, character: "Goki", style: "Human ver." },
                { id: 28, character: "Goki", style: "Plushie ver." },
                { id: 29, character: "No.7", style: "Normal" },
                { id: 30, character: "No.7", style: "Announcement" },
                { id: 31, character: "No.7", style: "Reading Aloud" },
                { id: 42, character: "Chibi Shiki Jii", style: "Normal" },
                { id: 43, character: "Ouka Miko", style: "Normal" },
                { id: 44, character: "Ouka Miko", style: "Second Form" },
                { id: 45, character: "Ouka Miko", style: "Loli" },
                { id: 46, character: "Sayo/SAYO", style: "Normal" },
                { id: 47, character: "Nurse Robot Type T", style: "Normal" },
                { id: 48, character: "Nurse Robot Type T", style: "Easygoing" },
                { id: 49, character: "Nurse Robot Type T", style: "Fearful" },
                { id: 50, character: "Nurse Robot Type T", style: "Secret Talk" },
                { id: 51, character: "†Holy Knight Kurezakura†", style: "Normal" },
                { id: 52, character: "Suzumatsu Shuji", style: "Normal" },
                { id: 53, character: "Kigashima Sourin", style: "Normal" },
                { id: 54, character: "Haruka Nana", style: "Normal" },
                { id: 55, character: "Nekotsukai All", style: "Normal" },
                { id: 56, character: "Nekotsukai All", style: "Calm" },
                { id: 57, character: "Nekotsukai All", style: "Cheerful" },
                { id: 58, character: "Nekotsukai Bii", style: "Normal" },
                { id: 59, character: "Nekotsukai Bii", style: "Calm" },
                { id: 60, character: "Nekotsukai Bii", style: "Shy" }
            ];

            // Populate Speaker ID Dropdown
            speakerStyles.forEach(speaker => {
                const option = document.createElement('option');
                option.value = speaker.id;
                option.textContent = `${speaker.id}: ${speaker.character} - ${speaker.style}`;
                speakerIdSelect.appendChild(option);
            });
            speakerIdSelect.value = currentSpeakerId; // Set initial value

            speakerIdSelect.addEventListener('change', (event) => {
                currentSpeakerId = parseInt(event.target.value, 10);
                console.log(`Speaker ID changed to: ${currentSpeakerId}`);
            });


            async function speakWithVoicevox(text, subtitle) {
                if (!text) {
                    aiStatus.textContent = 'Ready.';
                    return;
                }
                if (audioContext && audioContext.state === 'suspended') {
                    await audioContext.resume();
                } else if (!audioContext) {
                    audioContext = new (window.AudioContext || window.webkitAudioContext)();
                    // Ensure it's resumed if created fresh and suspended
                    if (audioContext.state === 'suspended') {
                        try {
                            await audioContext.resume();
                            console.log("AudioContext resumed in speakWithVoicevox after fresh creation.");
                        } catch (e) {
                            console.error("Error resuming new AudioContext in speakWithVoicevox:", e);
                        }
                    }
                }

                // triggerSpeechParticleEffect(); // This function was removed

                try {
                    // 1. Get audio query
                    const audioQueryResponse = await fetch(`${VOICEVOX_API_URL}/audio_query?text=${encodeURIComponent(text)}&speaker=${currentSpeakerId}`, {
                        method: 'POST',
                        headers: { 'Content-Type': 'application/json' },
                    });
                    if (!audioQueryResponse.ok) {
                        const errorData = await audioQueryResponse.json();
                        console.error("Voicevox audio_query error:", errorData);
                        throw new Error(`Voicevox audio_query failed: ${audioQueryResponse.status} - ${errorData.detail || 'Unknown error'}`);
                    }
                    const audioQuery = await audioQueryResponse.json();

                    // 2. Synthesize audio
                    const synthesisResponse = await fetch(`${VOICEVOX_API_URL}/synthesis?speaker=${currentSpeakerId}`, {
                        method: 'POST',
                        headers: { 'Content-Type': 'application/json' },
                        body: JSON.stringify(audioQuery)
                    });

                    if (!synthesisResponse.ok) {
                        const errorData = await synthesisResponse.json(); // Voicevox might return JSON error here
                        console.error("Voicevox synthesis error:", errorData);
                        throw new Error(`Voicevox synthesis failed: ${synthesisResponse.status} - ${errorData.detail || 'Unknown error'}`);
                    }

                    const audioBlob = await synthesisResponse.blob();
                    const audioUrl = URL.createObjectURL(audioBlob);
                    const audio = new Audio(audioUrl);
                    audio.play();

                    audio.onended = () => {
                        aiStatus.textContent = 'Ready.';
                        // Subtitles remain visible until next interaction
                    };
                    audio.onerror = (e) => {
                        console.error('Error playing audio:', e);
                        aiStatus.textContent = 'Error playing audio.';
                    };

                } catch (error) {
                    console.error('Error with Voicevox:', error);
                    aiStatus.textContent = 'Error with speech synthesis.';
                    subtitlesDiv.textContent = `Error: ${error.message}`; // Show error in subtitles
                    subtitlesDiv.style.display = 'block';
                }
            }

            // Initial status
            aiStatus.textContent = 'Ready. Waiting for input...';
        });
    </script>
</body>
</html>

